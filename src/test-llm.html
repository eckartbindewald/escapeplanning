<!DOCTYPE html>
<html>
<head>
  <title>LLM Test</title>
  <style>
    body {
      font-family: monospace;
      padding: 20px;
      max-width: 800px;
      margin: 0 auto;
      background: #1a1a1a;
      color: #fff;
      line-height: 1.6;
    }
    #output {
      background: #2a2a2a;
      padding: 15px;
      border-radius: 5px;
      white-space: pre-wrap;
      margin-top: 20px;
      border: 1px solid #444;
    }
    .error { color: #ff6b6b; }
    .success { color: #69db7c; }
    .info { color: #4dabf7; }
    .loading {
      display: inline-block;
      animation: pulse 1.5s infinite;
    }
    @keyframes pulse {
      0% { opacity: 0.5; }
      50% { opacity: 1; }
      100% { opacity: 0.5; }
    }
  </style>
</head>
<body>
  <h1>Testing LLM Model</h1>
  <div id="output">
    <div class="info loading">Loading ONNX runtime and model...</div>
  </div>

  <script type="module">
    import { pipeline, env } from '@xenova/transformers';
    import * as ort from 'onnxruntime-web';

    // Configure environment
    env.useBrowserCache = false;
    env.allowLocalModels = false;
    env.backends.onnx.wasm.wasmPaths = 'https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/';

    const output = document.getElementById('output');

    function log(message, type = '') {
      const line = document.createElement('div');
      line.textContent = message;
      if (type) line.className = type;
      output.appendChild(line);
      console.log(message);
    }

    async function testLLM() {
      output.textContent = ''; // Clear initial message
      log('Starting LLM test...', 'info');
      log('Model: Xenova/distilgpt2', 'info');
      
      try {
        log('\nInitializing ONNX runtime...', 'info');
        await ort.ready();
        log('ONNX runtime initialized successfully', 'success');

        log('\nInitializing pipeline...', 'info');
        const generator = await pipeline('text-generation', 'Xenova/distilgpt2', {
          progress_callback: (progress) => {
            if (progress.status === 'progress') {
              log(`Loading model: ${Math.round(progress.value * 100)}%`, 'info');
            }
          }
        });
        log('Pipeline initialized successfully', 'success');
        
        log('\nGenerating test response...', 'info');
        const result = await generator('Tell me a short story about a wise owl', {
          max_new_tokens: 50,
          temperature: 0.7,
          do_sample: true,
          no_repeat_ngram_size: 2
        });
        
        log('\nGenerated Response:', 'success');
        log(result[0].generated_text);
        log('\nTest completed successfully! âœ¨', 'success');
        
      } catch (error) {
        log('\nError during test:', 'error');
        log(error.toString(), 'error');
        if (error.stack) {
          log('\nStack trace:', 'error');
          log(error.stack, 'error');
        }
        log('\nTroubleshooting tips:', 'info');
        log('- Make sure your browser supports WebAssembly');
        log('- Check your internet connection');
        log('- Try refreshing the page');
      }
    }

    // Start the test with error handling
    testLLM().catch(error => {
      log('\nUnhandled error:', 'error');
      log(error.toString(), 'error');
    });
  </script>
</body>
</html>